<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [JOS-Admin] SWAT Team</TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:gchii%40mindspring.com">
   <LINK REL="Previous"  HREF="000125.html">
   <LINK REL="Next" HREF="000130.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[JOS-Admin] SWAT Team</H1>
    <B>Gilbert Carl Herschberger II</B> 
    <A HREF="mailto:gchii%40mindspring.com"
       TITLE="[JOS-Admin] SWAT Team">gchii@mindspring.com</A><BR>
    <I>Thu, 05 Aug 1999 10:52:17 -0400</I>
    <P><UL>
        <LI> Previous message: <A HREF="000125.html">[JOS-Admin] SWAT Team</A></li>
        <LI> Next message: <A HREF="000130.html">[JOS-Admin] SWAT Team</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#128">[ date ]</a>
              <a href="thread.html#128">[ thread ]</a>
              <a href="subject.html#128">[ subject ]</a>
              <a href="author.html#128">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>At 07:43 AM 8/5/99 -0500, you wrote:
&gt;<i>Helmut,
</I>&gt;<i>
</I>&gt;<i>If you will tell me at what time all of this happenned...
</I>
Late Sunday afternoon (4:00pm EDT), wiki was up and running. By Monday
morning (10:00am EDT), it displayed only a &quot;forbidden&quot; message. Thinking
that it was a temporary Internet glitch, I did not respond immediately.

Helmut should know when he changed the attribute of the CGI script. Helmut?

This matches your analysis that a web-bot found the link to a search page
or all pages link _and_ started worming its way through every page on wiki. 

&gt;<i>02 Aug: ~15000
</I>
15,000 requests is a *lot* of requests. All those requests are requests
from the outside, right? Those requests could not be performed from within
wiki. It could not have been a bug in our wiki script.

Helmut did the right thing, shutting down wiki the way he did. By changing
the attributes from execute to read-only, the wiki CGI script cannot be
executed. The web-bot started getting &quot;forbidden&quot; pages instead of pages
(with more links).

Helmut did the right thing, keeping wiki shut down until we understand what
happened. We are getting closer to understanding what happened, I think.
Has anyone checked to make certain that robots.txt is up-to-date? It should
have been updated concurrent with the search features. Otherwise, it was
just a matter of time before a web-bot got carried away with those search
pages.

And another thing. Because Helmut changed the attribute from execute to
read-only, we know it was probably not a run-away CGI script. A CGI program
that was already running would have kept on running even after the file
attribute was changed. Wiki doesn't need to use recursive calls to a CGI
script, like other websites do.

From the log, do we know where those request *came from*? I wouldn't be
surprised if 14,500 requests came from one IP address.

This may be an accidental &quot;denial of service&quot; attack. The HTTP server
should pose limit the number of requests coming from a single IP address.
Many sites limit requests from unknown IP addresses to 300 per hour, which
should not get in the way of human browsers. For known IP addresses, like
friendly web-bots, the limit varies based on time-of-day. This is a HTTP
service responsibility, neither CGI's or wiki's.


</pre>



<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI> Previous message: <A HREF="000125.html">[JOS-Admin] SWAT Team</A></li>
	<LI> Next message: <A HREF="000130.html">[JOS-Admin] SWAT Team</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#128">[ date ]</a>
              <a href="thread.html#128">[ thread ]</a>
              <a href="subject.html#128">[ subject ]</a>
              <a href="author.html#128">[ author ]</a>
         </LI>
       </UL>
</body></html>
